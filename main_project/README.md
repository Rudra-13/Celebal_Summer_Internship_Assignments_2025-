# Celebal Summer Internship Main Project - 2025

This project performs structured ingestion and processing of customer data using **PySpark on Databricks**, simulating the loading from Azure Data Lake Storage Gen2. The goal is to process various file formats and create **Delta tables**, validating they each contain **500 customer records**.

---

## ğŸ“‚ Project Structure


main_project/
â”œâ”€â”€ adls_gen2(simulated)/ # Customer data in various formats (CSV, JSON, Parquet, etc.)
â”œâ”€â”€ Scripts/ # PySpark scripts for loading and processing
â”œâ”€â”€ json_templates/ # ADLS linked service config (for ADF or Databricks mount)
â”œâ”€â”€ screenshots/ # Validation proof screenshots




---

## ğŸš€ Steps Covered (as per assignment)

1. **Load sample customer data** into simulated ADLS Gen2 (`adls_gen2(simulated)/`)
2. **Connect** to Databricks and read files using PySpark
3. **Create managed Delta tables** using:
   - **CTAS** for: `.avro`, `.parquet`, `.orc`, `.delta`
   - **Temporary View + CTAS** for: `.csv`, `.json`, `.tsv`, `.txt`, `.xml`, `.xlsx`
4. **Validate each Delta table** to ensure it contains **exactly 500 rows**

---

## ğŸ§  Script Descriptions (in `/Scripts`)

| Script File                     | Purpose                                                                 |
|---------------------------------|-------------------------------------------------------------------------|
| `load_to_managed_delta.py`      | Loads data into a **managed Delta table** (not external)               |
| `ctas_format_based.py`          | Handles CTAS for AVRO, PARQUET, ORC, and DELTA                         |
| `tempview_ctas_text_formats.py` | Reads CSV, JSON, TSV, TXT, XML, XLSX with schema â†’ TempView â†’ CTAS     |
| `validate_row_count.py`         | Generic function to validate Delta tables have **500 rows** each       |

---

## ğŸ“¦ Required Packages

- `pyspark`
- `openpyxl` or `com.crealytics.spark.excel` (for `.xlsx`)
- `spark-xml` (for `.xml` files)
- Databricks runtime with Delta Lake support

> Make sure you're running on Databricks or a PySpark environment with the above JARs installed.

---

## ğŸ–¼ï¸ Screenshots (Proof of Count = 500)

All Delta tables were successfully created and validated to contain **exactly 500 customer records**.

| Format     | Screenshot                      | Count |
|------------|----------------------------------|--------|
| CSV        | âœ… `customers_CSV_count.png`      | 500    |
| JSON       | âœ… `customers_JSON_count.png`     | 500    |
| TSV        | âœ… `customers_TSV_count.png`      | 500    |
| TXT        | âœ… `customers_TXT_count.png`      | 500    |
| XML        | âœ… `customers_XML_count.png`      | 500    |
| Others     | Validated via Spark console or logs | 500   |

---

## ğŸ§ª Running the Validation

```bash
# Run this script in Databricks or PySpark environment
python validate_row_count.py
